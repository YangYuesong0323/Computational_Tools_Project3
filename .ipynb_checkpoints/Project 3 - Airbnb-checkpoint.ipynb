{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Airbnb\n",
    "**This is the third of three mandatory projects to be handed in as part of the assessment for the course 02807 Computational Tools for Data Science at Technical University of Denmark, autumn 2019.**\n",
    "\n",
    "#### Practical info\n",
    "- **The project is to be done in groups of at most 3 students**\n",
    "- **Each group has to hand in _one_ Jupyter notebook (this notebook) with their solution**\n",
    "- **The hand-in of the notebook is due 2019-12-05, 23:59 on DTU Inside**\n",
    "\n",
    "#### Your solution\n",
    "- **Your solution should be in Python/PySpark**\n",
    "- **For each question you may use as many cells for your solution as you like**\n",
    "- **You should not remove the problem statements**\n",
    "- **Your notebook should be runnable, i.e., clicking [>>] in Jupyter should generate the result that you want to be assessed**\n",
    "- **You are not expected to use machine learning to solve any of the exercises**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "[Airbnb](http://airbnb.com) is an online marketplace for arranging or offering lodgings. In this project you will use Spark to analyze data obtained from the Airbnb website. The purpose of the analysis is to extract information about trends and patterns from the data.\n",
    "\n",
    "The project has two parts.\n",
    "\n",
    "### Part 1: Loading, describing and preparing the data\n",
    "There's quite a lot of data. Make sure that you can load and correctly parse the data, and that you understand what the dataset contains. You should also prepare the data for the analysis in part two. This means cleaning it and staging it so that subsequent queries are fast.\n",
    "\n",
    "### Par 2: Analysis\n",
    "In this part your goal is to learn about trends and usage patterns from the data. You should give solutions to the tasks defined in this notebook, and you should use Spark to do the data processing. You may use other libraries like for instance Pandas and matplotlib for visualisation.\n",
    "\n",
    "## Guidelines\n",
    "- Processing data should be done using Spark. Once data has been reduced to aggregate form, you may use collect to extract it into Python for visualisation.\n",
    "- Your solutions will be evaluated by correctness, code quality and interpretability of the output. This means that you have to write clean and efficient Spark code that will generate sensible execution plans, and that the tables and visualisations that you produce are meaningful and easy to read.\n",
    "- You may add more cells for your solutions, but you should not modify the notebook otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark session and define imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Project3_ys\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Loading, describing and preparing the data\n",
    "The data comes in two files. Start by downloading the files and putting them in your `data/` folder.\n",
    "\n",
    "- [Listings](https://files.dtu.dk/u/siPzAasj8w2gI_ME/listings.csv?l) (5 GB)\n",
    "- [Reviews](https://files.dtu.dk/u/k3oaPYp6GjKBeho4/reviews.csv?l) (9.5 GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "The data has multiline rows (rows that span multiple lines in the file). To correctly parse these you should use the `multiline` option and set the `escape` character to be `\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = spark.read.option('header', True).\\\n",
    "    option('inferSchema', True).\\\n",
    "    option('multiLine', True).\\\n",
    "    option('escape', \"\\\"\").csv('../data/listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = spark.read.option('header', True).\\\n",
    "    option('inferSchema', True).\\\n",
    "    option('multiLine', True).\\\n",
    "    option('escape', \"\\\"\").csv('../data/reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the data\n",
    "List the features (schema) and sizes of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for listings.csv\n",
    "listings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of listings.csv\n",
    "print(\"The number of rows in listings.csv : {}\".format(listings.count()))\n",
    "print(\"The number of columns in listings.csv : {}\".format(len(listings.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for reviews.csv\n",
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of reviews.csv\n",
    "print(\"The number of rows in reviews.csv : {}\".format(reviews.count()))\n",
    "print(\"The number of columns in reviews.csv : {}\".format(len(reviews.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data for analysis\n",
    "You should prepare two dataframes to be used in the analysis part of the project. You should not be concerned with cleaning the data. There's a lot of it, so it will be sufficient to drop rows that have bad values. You may want to go back and refine this step at a later point when doing the analysis.\n",
    "\n",
    "You may also want to consider if you can stage your data so that subsequent processing is more efficient (this is not strictly necessary for Spark to run, but you may be able to decrease the time you sit around waiting for Spark to finish things)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all NA values in listings and count the number of rows omitting those null values\n",
    "listings_na=listings.dropna()\n",
    "listings_na.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listings_filtered = listings.filter(f.col('price').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all NA values in reviews and count the number of rows omitting those null values\n",
    "reviews_na = reviews.dropna()\n",
    "reviews_na.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sampled 10% of the listings.csv and 5% of the reviews.csv\n",
    "# We write them into local disk, listings_sample.csv and reviews_sample.csv\n",
    "\n",
    "# No need to run now\n",
    "# sample=df.sample(False, 0.05, 33)\n",
    "# sample.coalesce(1).write.csv('data/names_here.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Analysis\n",
    "Use Spark and your favorite tool for data visualization to solve the following tasks.\n",
    "\n",
    "## The basics\n",
    "Compute and show a table with the number of listings and neighbourhoods per city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_city=listings.groupBy('city').\\\n",
    "    agg(f.countDistinct('id').alias(\"Distinct Listings\"),\\\n",
    "        f.countDistinct('neighbourhood_cleansed').alias(\"Distinct Neighbourhood\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_city.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the table above, you should choose a city that you want to continue your analysis for. The city should have mulitple neighbourhoods with listings in them.\n",
    "\n",
    "Compute and visualize the number of listings of different property types per neighbourhood in your city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if Copenhagen fulfills the requirement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_city.filter(f.col(\"city\") == \"Copenhagen\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yup, it does. We will choose Copenhagen for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract listings that are in Copenhagen\n",
    "copenhagen=listings.filter(f.col(\"city\") == \"Copenhagen\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and show with a table the number of listings per neighbourhood per property type in Copenhagen.\n",
    "copenhagen_groupby=copenhagen.groupBy('neighbourhood_cleansed','property_type').\\\n",
    "                    agg(f.countDistinct('id').alias(\"Listings Count\")).\\\n",
    "                    orderBy('neighbourhood_cleansed', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copenhagen_groupby.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prices\n",
    "Compute the minimum, maximum and average listing price in your city. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the schema, it can be seen that the price column is of type string \n",
    "# using regular expression, we clean the strings and then cast the column type to float\n",
    "copenhagen_cleanPrice = copenhagen.\\\n",
    "        withColumn('price', f.regexp_replace('price', '\\$', '')).\\\n",
    "        withColumn('price', f.regexp_replace('price', ',', '')).\\\n",
    "        withColumn('price', f.col('price').cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the minimum, maximum and average listing price in Copenhagen\n",
    "copenhagen_cleanPrice.select(f.min('price'), f.max('price'), f.avg('price')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and visualize the distribution of listing prices in your city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use groupby, count the number of listings at each price\n",
    "price_distribution = copenhagen_cleanPrice.groupby(\"price\").\\\n",
    "                        agg(f.countDistinct('id').alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a price list for histogram\n",
    "price_list=[]\n",
    "for row in price_distribution.collect():\n",
    "    price_list.extend([row['price']] * int(row['count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize price distribution using histogram. Adjust the bins and bin size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. bin size = 10\n",
    "plt.hist(price_list,bins=10)\n",
    "plt.title(\"Price distribution (10 bins)\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Number of listings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 13 bins, interval = 200, long tail (price>10000) ignored\n",
    "plt.hist(price_list,\n",
    "         bins = [200,400,600,800,1000,1200,1400,1600,1800,2000,4000,6000,8000,10000])\n",
    "plt.title(\"Price distribution (13 bins, price>10000 not included)\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Number of listings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 10 bins, interval = 200, long tail (price>4000) ignored\n",
    "plt.hist(price_list,\n",
    "         bins = [200,400,600,800,1000,1200,1400,1600,1800,2000,4000])\n",
    "plt.title(\"Price distribution (10 bins, price>4000 not included)\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Number of listings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of a listing is its rating divided by its price.\n",
    "\n",
    "Compute and show a dataframe with the 3 highest valued listings in each neighbourhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast rating and price column into float, then compute the value of listings and store the results in a new column\n",
    "copenhagen_value=copenhagen.\\\n",
    "        withColumn('review_scores_rating', f.col('review_scores_rating').cast('float')).\\\n",
    "        withColumn('price', f.regexp_replace('price', '\\$', '')).\\\n",
    "        withColumn('price', f.regexp_replace('price', ',', '')).\\\n",
    "        withColumn('price', f.col('price').cast('float')).\\\n",
    "        withColumn('value', f.col('review_scores_rating')/f.col('price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a window and sort the table by value\n",
    "value_window = Window.partitionBy('neighbourhood_cleansed').orderBy(f.desc('value'))\n",
    "ranked_df = copenhagen_value.withColumn('valueRank', f.rank().over(value_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the top 3 valued listing in each neighbourhood\n",
    "ranked_df.filter(f.col('valueRank') <= 3).\\\n",
    "    orderBy('neighbourhood_cleansed', f.desc('value')).\\\n",
    "    select('id','neighbourhood_cleansed','value','valueRank').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trends\n",
    "Now we want to analyze the \"popularity\" of your city. The data does not contain the number of bookings per listing, but we have a large number of reviews, and we will assume that this is a good indicator of activity on listings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and visualize the popularity (i.e., number of reviews) of your city over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join reviews and listings tables on listing id, and keep date and neighbourhood columns\n",
    "joined=reviews.select('listing_id','date').\\\n",
    "            join(copenhagen.select('id','neighbourhood_cleansed'), \n",
    "                 f.col('listing_id') == f.col('id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the year, month and date from the date column and store each in a separate column\n",
    "joined_clean=joined.withColumn('year', f.col('date').substr(1,4)).\\\n",
    "                    withColumn('month', f.col('date').substr(6,2)).\\\n",
    "                    withColumn('day', f.col('date').substr(9,2)).\\\n",
    "                    drop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# craete a cache for faster queries and operations\n",
    "joined_clean.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of reviews over the years\n",
    "gb_year=joined_clean.groupBy('year').count().\\\n",
    "                withColumn('year', f.col('year').cast('Integer')).\\\n",
    "                orderBy('year', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_year.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export and visualize\n",
    "year = [row['year'] for row in gb_year.collect()]\n",
    "review_count=[row['count'] for row in gb_year.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(year,review_count)\n",
    "plt.title(\"Number of reviews in Copenhagen (by year)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Review Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and visualize the popularity of neighbourhoods over time. If there are many neighbourhoods in your city, you should select a few interesting ones for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of reviews in each neighbourhood in each year\n",
    "gb_neighbor_year=joined_clean.groupBy('neighbourhood_cleansed', 'year').count().\\\n",
    "                withColumn('year', f.col('year').cast('Integer')).\\\n",
    "                orderBy(['neighbourhood_cleansed','year'], ascending=[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_neighbor_year.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary to store the information, where key is name of the neighbourhood and value is a list of tuples \n",
    "# {neighbourhood: [(year1, count1), ...], ...}\n",
    "dic={}\n",
    "for row in gb_neighbor_year.collect():\n",
    "    if row['neighbourhood_cleansed'] in dic:\n",
    "        dic[row['neighbourhood_cleansed']] += [(row['year'],row['count'])]\n",
    "    else:\n",
    "        dic[row['neighbourhood_cleansed']] = [(row['year'],row['count'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number of reviews in each neighbourhood in each year\n",
    "def plot():    \n",
    "    fig, ax = plt.subplots(4, 3, figsize=(30,20))\n",
    "    plt.subplots_adjust(wspace = 0.2, hspace = 0.7)\n",
    "    \n",
    "    for index,key in enumerate(dic.keys()):\n",
    "        row = index//3\n",
    "        col = index%3      \n",
    "        ax[row, col].set_title(key)\n",
    "        ax[row, col].set_xlabel(\"Year\")\n",
    "        ax[row, col].set_ylabel(\"Review Count\")\n",
    "        ax[row, col].plot([x[0] for x in dic[key]], [x[1] for x in dic[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"navy\">Comparing the yearly trends of the reviews, it can be seen that most neighbourhoods experience an upward-then-downward trend, where the number of reviews peaked at 2015-2016 and has been on an decline since then. The only exception is Valby, where the number of reviews increased steadily from 2013 to 2019.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and visualize the popularity of your city by season. For example, visualize the popularity of your city per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of reviews by month\n",
    "gb_month=joined_clean.groupBy('month').count().\\\n",
    "                withColumn('month', f.col('month').cast('Integer')).\\\n",
    "                orderBy('month', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_month.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export and visualize\n",
    "month = [row['month'] for row in gb_month.collect()]\n",
    "review_count=[row['count'] for row in gb_month.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(month,review_count)\n",
    "plt.title(\"Number of reviews in Copenhagen (by month)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Review Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see if there is any seasonality effects,\n",
    "# use groupby to count the number of reviews per month per year\n",
    "gb_year_month=joined_clean.groupBy('year','month').count().\\\n",
    "                withColumn('year', f.col('year').cast('Integer')).\\\n",
    "                withColumn('month', f.col('month').cast('Integer')).\\\n",
    "                orderBy(['year','month'], ascending=[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_year_month.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export and visualize\n",
    "review_count=[row['count'] for row in gb_year_month.collect()]\n",
    "plt.title(\"Number of reviews in Copenhagen (by year & month)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Review Count\")\n",
    "plt.plot(review_count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"navy\">As can be seen from the plot, there is no clear seasonality in the first 40 months, perhaps due to the few number of users as Airbnb was just launched not long ago. Seasonality becomes more pronounced from the 40th month onwards, where a cyclic pattern is observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews\n",
    "In this part you should determine which words used in reviews that are the most positive. \n",
    "\n",
    "The individual reviews do not have a rating of the listing, so we will assume that each review gave the average rating to the listing, i.e., the one on the listing.\n",
    "\n",
    "You should assign a positivity weight to each word seen in reviews and list the words with the highest weight. It is up to you to decide what the weight should be. For example, it can be a function of the rating on the listing on which it occurs, the number of reviews it occurs in, and the number of unique listings for which it was used to review.\n",
    "\n",
    "Depending on your choice of weight function, you may also want to do some filtering of words. For example, remove words that only occur in a few reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"navy\">**To fulfil the above requirements, our group has come up with a plan that has the following steps:**\n",
    "\n",
    "1. join reviews and listings on id, keep the comments and ratings column\n",
    "<br>\n",
    "<br>\n",
    "2. clean and tokenize the comments column\n",
    "<br>\n",
    "<br>\n",
    "3. for each word token, compute its average rating and store this information in a dictionary. This is achieved by:\n",
    "    1. initializing a dictionary that looks like this: `{token1 : [sum_ratings, total_num_occurrences], ...}`\n",
    "    2. iterating through every token, and update it accordingly\n",
    "        - e.g. we start off by having a dictionary {\"good\": (70, 10)}. This means that so far, the word \"good\" has a cumulative rating of 70, and this word has occurred for 10 times already. \n",
    "        - Should we encounter the word \"good\" again in a review whose rating is 8, we will update the dictionary accordingly, so that the new dictionary will become {\"good\":(78, 11)}\n",
    "    3. finally, computing the average rating of each token by using sum_ratings/total_num_occurrences\n",
    "        - e.g. in the previous example {\"good\":(78, 11)}, the average rating of the word \"good\" will be 78/11 = 7.09\n",
    "<br>\n",
    "<br>\n",
    "4. sort the dictionary and return words with the highest ratings</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pip packages gensim in the current Jupyter kernel\n",
    "# gensim is a Natural Language Processing library\n",
    "import sys\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.parsing.preprocessing as gsp\n",
    "from gensim import utils\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up filters for text cleaning\n",
    "filters = [\n",
    "           gsp.strip_tags,  # Conversion to lowercase\n",
    "           gsp.strip_punctuation, # Removal of punctuations\n",
    "           gsp.strip_multiple_whitespaces, # Removal of extra spaces\n",
    "           gsp.strip_numeric, # Removal of integers, numbers\n",
    "           gsp.remove_stopwords, # Removal of stop words (like ‘and’, ‘to’, ‘the’ etc)\n",
    "           # gsp.stem_text # Stemming (Conversion of words to root form)\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join reviews and listings tables on listing id, and keep rating and comments columns\n",
    "pos_table = listings.select('id','review_scores_rating').\\\n",
    "                join(reviews.select('listing_id','comments'), \n",
    "                     f.col('id') == f.col('listing_id')).drop('listing_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where either rating or comments is null\n",
    "pos_table = pos_table.dropna().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to preprocess the raw comments\n",
    "def preprocess_comments(x):\n",
    "    x = x.lower() # set to lower case\n",
    "    x = utils.to_unicode(x) # convert to unicode\n",
    "    for f in filters:\n",
    "        x = f(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pre-process_comments() as a udf\n",
    "preprocess_comments_udf = f.udf(preprocess_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"purple\">YS's implementation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = pos_table.sample(False, 0.01, 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocess_comments_udf to the comments column of the spark dataframe\n",
    "pos_table_processed = pos_table.withColumn('processed_comments', \n",
    "                                preprocess_comments_udf(f.col('comments')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Apache Spark to tokenize the words\n",
    "tokenizer = Tokenizer(inputCol=\"processed_comments\", outputCol=\"tokens\")\n",
    "pos_table_tokens = tokenizer.transform(pos_table_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a dictionary that is used for storing and computing the positivity score for each word\n",
    "ratings_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ratings(row):\n",
    "    rating = row[1] # rating is the 1st column (0-index based) in the spark dataframe \n",
    "    tokens = row[4] # tokens is the 4th column (0-index based) in the spark dataframe\n",
    "    for token in tokens:\n",
    "        if token not in ratings_dic:\n",
    "            ratings_dic[token] = [0,0] # [m, n] where m is the sum of scores & n is the number of occurrences of this token\n",
    "        ratings_dic[token][0] += int(rating)\n",
    "        ratings_dic[token][1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for row in pos_table_tokens.collect():\n",
    "    compute_ratings(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(ratings_dic, open('rating_dictionary.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"purple\">ZY's implementation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocess_comments_udf to the comments column of the spark dataframe\n",
    "pos_table = pos_table.withColumn('processed_comments', preprocess_comments_udf(f.col('comments')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Apache Spark to tokenize the words\n",
    "tokenizer = Tokenizer(inputCol=\"processed_comments\", outputCol=\"tokens\")\n",
    "pos_table = tokenizer.transform(pos_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_table.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the scores of each list of tokens into a list\n",
    "scores_list = pos_table.select(\"review_scores_rating\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ratings in scores_list from string to integer\n",
    "for index, score in enumerate(scores_list):\n",
    "    try:\n",
    "        scores_list[index] = int(score)\n",
    "        \n",
    "    # if cannot be converted to integer, change this score to a dummy value -999 \n",
    "    except:\n",
    "        scores_list[index] = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(scores_list, open('scores_list.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('scores_list.p', 'rb') as fp:\n",
    "    scores_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# extract the tokens into a list of lists\n",
    "tokens_list = pos_table.select(\"tokens\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a dictionary that is used for storing and computing the positivity score for each word\n",
    "ratings_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the tokens and build a dictionary that stores each token's sum of scores and its number of occurrences\n",
    "for index, tokens in enumerate(tokens_list):\n",
    "    score = scores_list[index]\n",
    "    if score == -999: # bad entries\n",
    "        continue # ignore this record and proceed with the next\n",
    "    for token in tokens:\n",
    "        if token not in ratings_dic:\n",
    "            ratings_dic[token] = [0,0] # [m, n] where m is the sum of scores & n is the number of occurrences of this token\n",
    "        ratings_dic[token][0] += score # increment the total scores of this token by 1\n",
    "        ratings_dic[token][1] += 1 # increment the occurrences of this token by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a dictionary that is used for storing the average rating for each word\n",
    "ave_ratings_dic = {}\n",
    "\n",
    "# iterate through the key-value pairs in ratings_dic and build a dictionary that stores each token's average rating\n",
    "for key, value in ratings_dic.items():\n",
    "    \n",
    "    # filter the dictionary and remove words that occur below a certain number of times\n",
    "    threshold = 500 # remove words that occur fewer than 500 times across all reviews\n",
    "    if value[1] < threshold: \n",
    "        continue\n",
    "    \n",
    "    ave_rating = value[0]/value[1]\n",
    "    ave_ratings_dic[key] = ave_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dictionary based on each token's average rating in descending order\n",
    "# output a list of tuples, with the first element of each tuple being the token, and the second being its average score\n",
    "sorted_ave_ratings = sorted(ave_ratings_dic.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enquire the top 100 most positive words in the reviews\n",
    "sorted_ave_ratings[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. select id and rating from listings\n",
    "2. select id and comment from reviews\n",
    "3. join two on id\n",
    "4. define user defined function udf e.g.\n",
    "5. use udf to clean comment column --> remove stop words, common words blah blah, stemming maybe? stemming may be too computationally expensive, we could omit.\n",
    "\n",
    "6. store in a dic, key == rating , value == comment. if same rating, just concat comments. after iterations, value should be very long (few comments concat)\n",
    "\n",
    "7. tokenize comment, assign rate to each token and store avg rating for each word in a new dic. Lets say original dic is {7 : \"it is good\", 8: \"it looks good and fantastic\"}, the new dic will be {'good' : (7+8)/2 = 7.5, \"fantastic\" : 8}\n",
    "    - <font color=\"red\">searching through each word in the original dic ({7 : \"it is good\", 8: \"it looks good and fantastic\"}) and computing its ave rating might be computationally expensive(?). I'm thinking if we can obtain the new dic ({'good' : (7+8)/2 = 7.5, \"fantastic\" : 8}) directly from the raw text. \n",
    "    - So basically we tokenize a review first, and for every token that is an adjective (see my comment in the next code cell under bullet point #2), append it to a dictionary with its appropriate score and occurrence. e.g. we have a dictionary that is {\"good\": (70, 10)}, meaning that so far, the word \"good\" has a cumulative rating of 70, and this word has occurred for 10 times already. If we encounter the word \"good\" again in a review whose rating is 8, we will update the dictionary accordingly, so that the new dictionary will become {\"good\":(78, 11)}. Lookup is O(1) and from this dictionary, we can output the final positivity score of a word directly simply by using 78/11. What do you think?</font>\n",
    "<br>\n",
    "<br>\n",
    "8. return the highest score words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a few concerns:\n",
    "1. too many words invloved, i am not sure if a dic is appropriate, maybe should use streaming sketch, i do not know..\n",
    "    - <font color=\"red\">agreed. anyway preprocessing needs to be done in Spark so no choice</font>\n",
    "2. only considered listing rating in positivity weight. \"For example, it can be a function of the rating on the listing on which it occurs, the number of reviews it occurs in, and the number of unique listings for which it was used to review.\" maybe consider others, keep a counter on how many times this word appears, below a threshold then drop. This may help with point 1.\n",
    "    - <font color=\"red\">im not sure if this is what you mean, but to me it sounds a bit like the TF part of tfidf right? To help reduce the memory and also increase the accuracy, I'm thinking of only retaining the adjectives in each review, as they are the ones that will give insight into how positive the listing is. I've done 词性分析 on Chinese text using jieba, pretty sure there are libraries out there that perform similar tasks on English text too. Will do a bit of research on this part later on.</font>\n",
    "3. yes, memory space is a concern...can we store so many words in memory?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
